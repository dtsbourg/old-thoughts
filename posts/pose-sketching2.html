<!doctype html>
<html lang="en">

<head>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>dtsbourg / Thoughts | Pose Sketching for the Control of a Humanoid Robot - Part 2</title>
  <meta name="description" content="The visual servoing problem.">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Pose Sketching for the Control of a Humanoid Robot - Part 2">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://dtsbourg.github.io/thoughts/posts/pose-sketching2">
  <meta property="og:description" content="The visual servoing problem.">
  <meta property="og:site_name" content="dtsbourg / Thoughts">
  <meta property="og:image" content="https://dtsbourg.github.io/thoughts/thoughts/assets/ico/favicon.ico">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="https://dtsbourg.github.io/thoughts/posts/pose-sketching2">
  <meta name="twitter:title" content="Pose Sketching for the Control of a Humanoid Robot - Part 2">
  <meta name="twitter:description" content="The visual servoing problem.">
  <meta name="twitter:image" content="https://dtsbourg.github.io/thoughts/thoughts/assets/ico/favicon.ico">

  <link rel="apple-touch-icon" sizes="180x180" href="/thoughts/assets/ico/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/thoughts/assets/ico/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/thoughts/assets/ico/favicon-16x16.png">
  <link rel="mask-icon" href="/thoughts/assets/ico/safari-pinned-tab.svg" color="#5bbad5">
  <link href="https://dtsbourg.github.io/thoughts/feed.xml" type="application/rss+xml" rel="alternate" title="dtsbourg / Thoughts Last 10 blog posts" />

  

  
    <link rel="stylesheet" href="/thoughts/assets/light.css">

  
</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav appear">
  <a href="/thoughts/" class="header-logo" title="dtsbourg / Thoughts">dtsbourg / Thoughts</a>
  <ul class="header-links">
    
    
      <li>
        <a href="https://twitter.com/dtsbourg" rel="noreferrer noopener" target="_blank" title="Twitter">
          <span class="icon icon-twitter"></span>
        </a>
      </li>
    
    
    
      <li>
        <a href="https://github.com/dtsbourg" rel="noreferrer noopener" target="_blank" title="GitHub">
          <span class="icon icon-github"></span>
        </a>
      </li>
    
    
    
    
      <li>
        <a href="https://www.linkedin.com/in/dylan-bourgeois-319ab294" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <span class="icon icon-linkedin"></span>
        </a>
      </li>
    
    
    
    
      <li>
        <a href="/thoughts/feed.xml" rel="noreferrer noopener" target="_blank" title="RSS">
          <span class="icon icon-rss"></span>
        </a>
      </li>
    
  </ul>
</nav>

        <article class="article appear">
          <header class="article-header">
            <h1>Pose Sketching for the Control of a Humanoid Robot - Part 2</h1>
            <p>The visual servoing problem.</p>
            <div class="article-list-footer">
              <span class="article-list-date">
                February 8, 2018
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
                
                
                  8 minute read
                
              </span>
              <span class="article-list-divider">-</span>
              <div class="article-list-tags">
                
                  <a href="/thoughts/tag/robot">robot</a>
                
                  <a href="/thoughts/tag/project">project</a>
                
                  <a href="/thoughts/tag/epfl">epfl</a>
                
              </div>
            </div>
          </header>

          <div class="article-content">
            <h2 id="introduction">Introduction</h2>

<p>In this series of posts, I will be writing up about some work done in the
context of a semester project counting for my degree from <a href="https://epfl.ch">EPFL</a>.
This is meant more as a record than an in-depth tutorial, but don’t hesitate to
drop me a line if you would like to see more details.
This work was done under supervision by <a href="https://calinon.ch">Sylvain Calinon</a>,
senior researcher in the Robot Learning &amp; Interaction Group at <a href="https://idiap.ch">Idiap Research Institute</a>
in Martigny, Switzerland.</p>

<p>The main focus of the project was to investigate new ways of controlling
high degree of freedom robots, with an application to the Humanoid robot Baxter.</p>

<h2 id="visual-servoing">Visual servoing</h2>

<p>Visual servoing, or vision-based control, uses visual feedback to control a robot. In particular, eye-in-hand, or <em>end-point open-loop control</em> is a setting where the camera is free and tracks the relative position of the arm with respect to the points in the world coordinates.</p>

<p>We intend to apply here a version of visual servoing in which the control law is based on minimising the error between the observed pose and the desired pose by minimising the error in the image (i.e. working in the pixel space directly). This is called Image-Based Visual Servoing (IBVS) proposed by Weiss et al. <sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>.</p>

<h2 id="mapping-the-task-space-to-the-pixel-space">Mapping the task space to the pixel space</h2>

<p>As is often the case in control settings, we need to find a mapping between different coordinate spaces since the commands to the robot are given in its own local referential (the joint angles) but the effects of these commands (e.g. moving the arm) are to be measured in the global referential (for example xyz coordinates of the robot’s end-effector) if they are to have any relevance to the task at hand.</p>

<p><img src="/thoughts/assets/pose-sketch/vis_lin.png" alt="Interaction Model" />
<em>Figure 1: Linearised optical model</em></p>

<p>Since our objective is to have a map between any pixel on our interface and points in the real world (what point in the real world is this pixel showing me, hence if I click on this pixel, what point in the real world am I interacting with ?). Obviously the difficulty resides in the fact that there is not a unique solution in the real world for each pixel on the interface. This is already obvious with a simple ray-tracing model. Take a pixel on the screen and trace a straight line forward from the interface. Every single point on this line is mapped to the same pixel on the screen so there is no way (for now) to know which of these points we were aiming for when clicking on this pixel. One way to get around this problem is to trace not one but two lines, coming from the interface at different positions but from the same point on the interface. The intersection of these two lines defines a unique point in space for this pixel.</p>

<p><img src="/thoughts/assets/pose-sketch/vis_piece_lin.png" alt="Interaction Model" />
<em>Figure 2: Piece-wise linearised optical model</em></p>

<p>This ray-tracing method is the simplest way to get a mapping from point to pixel and is quite useful for experimentation in a simple setting. Unfortunately it is unusable in a real setting, simply because this is not at all how lenses work, with the error of this model growing quadratically with the distance to the interface. It assumes there is a 1-1 mapping in scale between the screen and the objects (no distortion, no magnification), where using a lens allows us to view large scenes on a small screen. To circumvent this we can use a pinhole camera model that projects straight lines from a focal point behind the interface, but this also assumes the absence of distortion. We could also apply some form of linearisation which approximates this simple camera model but provides an easer geometrical framework (piece-wise linear) by stacking arbitrarily thin chunks of rays.</p>

<p><img src="/thoughts/assets/pose-sketch/pinholeCamera.png" alt="Interaction Model" />
<em>Figure 3: Pinhole camera model</em></p>

<p>Thankfully, optics provides us with a simple way to get over these issues by defining a projective matrix for the optical system of the camera. All we need to know are some basic lens characteristics: <script type="math/tex">f_x</script> and <script type="math/tex">f_y</script> are the respective focal lengths on axis x and y of the lens, <script type="math/tex">s</script> is the axis skew (which causes distortion in the image), <script type="math/tex">x_0</script> and <script type="math/tex">y_0</script> are the displacements of the studied point (in our case, where the user clicked on the screen). Note that these parameters are expressed in pixel units, meaning the distances are not taken into account. To model this dependency we can use a scaled version, dividing the values by the factor of the sensor sizes in pixels and mm.</p>

<script type="math/tex; mode=display">% <![CDATA[
\mathbf{K} = \begin{bmatrix} f_x & s   & x_0 \\ 0   & f_y & y_0 \\ 0   & 0   & 1 \end{bmatrix} %]]></script>

<p>However, this method is incomplete. We only get the matrix <script type="math/tex">K</script> which models the lens, and not the camera. For this we simply incorporate an extrinsic matrix (as opposed to the intrinsic matrix <script type="math/tex">\mathbf{K}</script>), modelling the pose of the camera in the world space, in generalised coordinates, comprised of a rotation <script type="math/tex">\mathbf{R}</script> and a translation <script type="math/tex">t</script>.</p>

<script type="math/tex; mode=display">\mathbf{P} = \mathbf{K} \cdot \begin{bmatrix} \mathbf{R} | t \end{bmatrix} \,\,\,\,,</script>

<p>Some methods have been proposed to learn such a mapping, such as deriving it online as in <sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>, or by calibrating the system with a point known by both the robot and the camera (getting an intermediate referential). In practice, this is the simplest way to find it. In our experiments we calibrated with the robot’s end effector as a known position, but previous attempts have defined arbitrary intermediate referentials for calibration.</p>

<h2 id="controlling-the-robot-from-the-pixel-space">Controlling the robot from the pixel space</h2>

<p>So once we have learnt these maps, we can finally use them to do something useful (well, at least fun): moving robots around ! Let’s define a simple robot as in Fig. 4. The arm of the robot is defined by two joint angles (<script type="math/tex">\mathbf{q} = [q_1\, \,q_2]^T</script>), and the end-effector’s position in the robot space is denoted <script type="math/tex">\mathbf{x_R}</script>.</p>

<p><img src="/thoughts/assets/pose-sketch/pix_control.png" alt="Interaction Model" />
<em>Figure 4: Controlling in the pixel space</em></p>

<p>Following our previous argument, we learn a mapping <script type="math/tex">F_c</script> which brings a point <script type="math/tex">\mathbf{x}</script> in world space to a point <script type="math/tex">\mathbf{p}</script> in the pixel space (on the screen). We could use the simple method of ray-tracing but as detailed earlier this method has severe shortcomings.</p>

<script type="math/tex; mode=display">F_c(\mathbf{x_R}) = \mathbf{p}</script>

<p>We also need to know how the end-effector get its position based on the joint angles of the robot. This is the kinematic model of the robot, and can be formalized as:</p>

<script type="math/tex; mode=display">F(\mathbf{q}) = \mathbf{x_R}</script>

<p>Well the rest is now straighforward: we have two mappings: one to go from the joint angles (what we want to control) to the end-effector’s position in space, and another from the end-effector onto the screen. We can compose the two to get a direct relationship between a position on the screen and a joint angle !</p>

<script type="math/tex; mode=display">\mathbf{p} = F_c(F(\mathbf{q}))</script>

<p>We can now define a control law that will minimise the error in the pixel space between the desired position <script type="math/tex">\hat{\mathbf{p}}</script> and the current position <script type="math/tex">\mathbf{p}</script> by controlling the joint angles <script type="math/tex">\mathbf{q}</script>. This can be obtained by applying the chain rule to the previous mapping, to obtain:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} \frac{\partial{\mathbf{p}}}{\partial{\mathbf{q}}} &= \frac{\partial ({F_c \circ F}) }{\partial{\mathbf{q}}} \\ & = \frac{\partial F}{\partial \mathbf{q}} \cdot \frac{\partial F_c}{\partial \mathbf{q}} \Bigg\rvert_{F(\mathbf{q})} \\ & = \mathbf{J} \cdot \mathbf{J}_c \big\rvert_{F(\mathbf{q})} \end{align} %]]></script>

<p>where <script type="math/tex">\mathbf{J}</script> is the Jacobian of <script type="math/tex">F</script>, mapping velocities from the joint-angles to the end-effector, and <script type="math/tex">\mathbf{J_c}</script> is the Jacobian of <script type="math/tex">F_c</script>, mapping velocities from the end-effector to the pixel space.
The final control law is obtained by computing the pseudo-inverse (<script type="math/tex">^+</script>) of this relation to finally get, for a simple PD controller (the gain <script type="math/tex">k_p</script> controls how aggressively we want to reach the solution):</p>

<script type="math/tex; mode=display">\dot{\mathbf{q}} = (\mathbf{J} \cdot \mathbf{J}_c \big\rvert_{F(\mathbf{q})} )^{+} \cdot k_p (\mathbf{p} - \hat{\mathbf{p}})</script>

<h2 id="visual-servoing-with-baxter">Visual servoing with Baxter</h2>

<p>We now have everything at our disposal to use this in the real world. We implemented the ideas developed earlier in the context of an Android app, running Tango (RIP) for the visual part, and ROS to communicate with the Baxter robot. After many hurdles, there is finally a cool video to show.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Ju-hmc45vpU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>

<h3 id="intrinsic-matrix-and-camera-parameters">Intrinsic matrix and Camera parameters</h3>

<p>As we have seen the intrinsic matrix is entirely defined by camera parameters. In our case we were able to retrieve them from Tango, letting us plugin the desired values to the matrix <script type="math/tex">\mathbf{K}</script>.</p>

<h3 id="extrinsic-matrix-and-calibration">Extrinsic matrix and Calibration</h3>

<p>We also have to learn the extrinsic matrix so we can figure out where the camera and robot are relative to each other. For this we used the simplest method available: since the phone’s pose as given by Tango was relative to the start-up position, we simply start-up the app at a position that is known from the robot, such as its end-effector. We can see this step in Fig. 5.</p>

<p><img src="/thoughts/assets/pose-sketch/calib.jpg" alt="Interaction Model" />
<em>Figure 5: Calibration step</em></p>

<h3 id="visual-interaction">Visual interaction</h3>

<p>We can now click on the interface to give the robot commands. The pipeline we described <strong>computes a direct mapping from the clicked position to joint angles</strong>. As mentioned there can be several solutions to the inverse map, so it is sometimes necessary to define several points, with there solutions being averaged out to give the final command for the end-effector.</p>

<p><img src="/thoughts/assets/pose-sketch/control.jpg" alt="Interaction Model" />
<em>Figure 6: Controlling in the pixel space</em></p>

<h2 id="final-thoughts">Final thoughts</h2>

<p>We present here a way of controlling the robot using information directly on the device (controlling in the pixel space), which opens a new set of intersecting control methods but also providing feedback in the same space as the interaction. This write-up is far from extensive and needs quite a bit of refining, but I hope you found it fun to follow along.</p>

<p>If you find mistakes, have comments or concerns, don’t hesitate to ping me !</p>

<p>I’d really like to thank <a href="https://calinon.ch">Sylvain Calinon</a> for supervising me during this project, as well as <a href="https://www.idiap.ch/~epignat/">Emmanuel Pignat</a> who really helped salvage the results.</p>

<h2 id="resources">Resources</h2>

<p>The code for the Android application is available on <a href="https://gitlab.idiap.ch/dbourgeois/PoseSketch/">Gitlab</a>. The presentation of this work can be found on <a href="https://www.youtube.com/watch?v=Ki5OA4gDldE">Youtube</a>, where I describe in a bit more detail the work that was done at IDIAP. If you are very interested, the final report for this project is also <a href="https://drive.google.com/file/d/1_X5A5q5ZO31dRIHeF4nbjvBRd7gWkBhQ/view">available</a>. All of this surely needs a lot of improvements, but that’s what we’re all here for aren’t we ? :)</p>

<hr />
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>L. Weiss, A. Sanderson, and C. Neuman. “Dynamic sensor-based control of robots with visual feedback”. In: IEEE Journal on Robotics and Automation 3.5 (1987), pp. 404–417. issn: 0882-4967. doi: 10.1109/JRA.1987.1087115. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Andreas Ruf and Radu Horaud. “Visual Servoing of Robot Manipulators Part I: Projective Kinematics”. In: The International Journal of Robotics Research 18.11 (1999), pp. 1101–1118. doi: 10.1177/02783649922067744. eprint: https://doi.org/10.1177/02783649922067744. url: https://doi.org/10.1177/02783649922067744.s <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

          </div>

          <div class="article-share">
            
            
            <a href="https://twitter.com/home?status=Pose+Sketching+for+the+Control+of+a+Humanoid+Robot+-+Part+2 - https://dtsbourg.github.io/thoughts/posts/pose-sketching2 by @dtsbourg" title="Share on Twitter" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M492 109.5c-17.4 7.7-36 12.9-55.6 15.3 20-12 35.4-31 42.6-53.6 -18.7 11.1-39.4 19.2-61.5 23.5C399.8 75.8 374.6 64 346.8 64c-53.5 0-96.8 43.4-96.8 96.9 0 7.6 0.8 15 2.5 22.1 -80.5-4-151.9-42.6-199.6-101.3 -8.3 14.3-13.1 31-13.1 48.7 0 33.6 17.2 63.3 43.2 80.7C67 210.7 52 206.3 39 199c0 0.4 0 0.8 0 1.2 0 47 33.4 86.1 77.7 95 -8.1 2.2-16.7 3.4-25.5 3.4 -6.2 0-12.3-0.6-18.2-1.8 12.3 38.5 48.1 66.5 90.5 67.3 -33.1 26-74.9 41.5-120.3 41.5 -7.8 0-15.5-0.5-23.1-1.4C62.8 432 113.7 448 168.3 448 346.6 448 444 300.3 444 172.2c0-4.2-0.1-8.4-0.3-12.5C462.6 146 479 129 492 109.5z"/></svg>
            </a>
            <a href="https://www.facebook.com/sharer/sharer.php?u=https://dtsbourg.github.io/thoughts/posts/pose-sketching2" title="Share on Facebook" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M288 192v-38.1c0-17.2 3.8-25.9 30.5-25.9H352V64h-55.9c-68.5 0-91.1 31.4-91.1 85.3V192h-45v64h45v192h83V256h56.4l7.6-64H288z"/></svg>
            </a>
            <a href="https://plus.google.com/share?url=https://dtsbourg.github.io/thoughts/posts/pose-sketching2" title="Share on Google+" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 128 128"><path d="M40.7 55.9v16.1c0 0 15.6 0 22 0C59.2 82.5 53.8 88.2 40.7 88.2c-13.3 0-23.7-10.8-23.7-24.2s10.4-24.2 23.7-24.2c7.1 0 11.6 2.5 15.8 5.9 3.3-3.3 3.1-3.8 11.6-11.9 -7.2-6.6-16.8-10.6-27.4-10.6C18.2 23.3 0 41.5 0 64c0 22.5 18.2 40.7 40.7 40.7 33.6 0 41.8-29.3 39-48.8H40.7zM113.9 56.7V42.6h-10.1v14.1H89.4v10.1h14.5v14.5h10.1V66.8H128V56.7H113.9z"/></svg>
            </a>
            <a href="http://news.ycombinator.com/submitlink?url=https://dtsbourg.github.io/thoughts/posts/pose-sketching2" title="Share on HN" rel="noreferrer noopener" target="_blank">
                <svg xmlns="http://www.w3.org/2000/svg" aria-label="Hacker News" role="img" viewBox="0 0 512 512"><rect width="512" height="512" fill="#ADA8A8" rx="15%"/><path fill="#fff" d="M124 91h51l81 162 81-164h51L276 293v136h-40V293z"/></svg>            
            </a>
          </div>

          
        </article>
        <footer class="footer appear">
  <p>
    I'm
    <a href="https://dtsbourg.github.io" title="About me">Dylan Bourgeois</a>. I study robots and their brains.
  </p>
</footer>

      </div>
    </div>
  </main>
  
  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-65856095-1','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async></script>


<script src="/thoughts/assets/vendor.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



  <script src="/thoughts/assets/webfonts.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>




  <script src="/thoughts/assets/scrollappear.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script src="/thoughts/assets/application.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
