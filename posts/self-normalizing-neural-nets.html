<!doctype html>
<html lang="en">

<head>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>dtsbourg / Thoughts | [NOTES] Self-Normalizing Neural Networks (SELU)</title>
  <meta name="description" content="by G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="[NOTES] Self-Normalizing Neural Networks (SELU)">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://dtsbourg.github.io/thoughts/posts/self-normalizing-neural-nets">
  <meta property="og:description" content="by G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter">
  <meta property="og:site_name" content="dtsbourg / Thoughts">
  <meta property="og:image" content="https://dtsbourg.github.io/thoughts/thoughts/assets/ico/favicon.ico">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="https://dtsbourg.github.io/thoughts/posts/self-normalizing-neural-nets">
  <meta name="twitter:title" content="[NOTES] Self-Normalizing Neural Networks (SELU)">
  <meta name="twitter:description" content="by G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter">
  <meta name="twitter:image" content="https://dtsbourg.github.io/thoughts/thoughts/assets/ico/favicon.ico">

  <link rel="apple-touch-icon" sizes="180x180" href="/thoughts/assets/ico/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/thoughts/assets/ico/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/thoughts/assets/ico/favicon-16x16.png">
  <link rel="mask-icon" href="/thoughts/assets/ico/safari-pinned-tab.svg" color="#5bbad5">
  <link href="https://dtsbourg.github.io/thoughts/feed.xml" type="application/rss+xml" rel="alternate" title="dtsbourg / Thoughts Last 10 blog posts" />

  

  
    <link rel="stylesheet" href="/thoughts/assets/light.css">

  
</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav appear">
  <a href="/thoughts/" class="header-logo" title="dtsbourg / Thoughts">dtsbourg / Thoughts</a>
  <ul class="header-links">
    
    
      <li>
        <a href="https://twitter.com/dtsbourg" rel="noreferrer noopener" target="_blank" title="Twitter">
          <span class="icon icon-twitter"></span>
        </a>
      </li>
    
    
    
      <li>
        <a href="https://github.com/dtsbourg" rel="noreferrer noopener" target="_blank" title="GitHub">
          <span class="icon icon-github"></span>
        </a>
      </li>
    
    
    
    
      <li>
        <a href="https://www.linkedin.com/in/dylan-bourgeois-319ab294" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <span class="icon icon-linkedin"></span>
        </a>
      </li>
    
    
    
    
      <li>
        <a href="/thoughts/feed.xml" rel="noreferrer noopener" target="_blank" title="RSS">
          <span class="icon icon-rss"></span>
        </a>
      </li>
    
  </ul>
</nav>

        <article class="article appear">
          <header class="article-header">
            <h1>[NOTES] Self-Normalizing Neural Networks (SELU)</h1>
            <p>by G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter</p>
            <div class="article-list-footer">
              <span class="article-list-date">
                March 23, 2018
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
                
                
                  3 minute read
                
              </span>
              <span class="article-list-divider">-</span>
              <div class="article-list-tags">
                
                  <a href="/thoughts/tag/ml">ml</a>
                
                  <a href="/thoughts/tag/deeplearning">deeplearning</a>
                
                  <a href="/thoughts/tag/notes">notes</a>
                
              </div>
            </div>
          </header>

          <div class="article-content">
            <p><a href="https://arxiv.org/abs/1706.02515">arXiv</a></p>

<h2 id="what">What?</h2>

<p>Proposes a new activation function allowing the robust training of very
deep vanilla neural networks.</p>

<h2 id="why">Why?</h2>

<p>Most of the successes of Deep Learning have come either from Recurrent (RNN)
or Convolutional (CNN) forms: they are stable through weight-sharing.
Vanilla Neural Networks suffer from training
instabilities mainly due to SGD being unstable after a few layers (vanishing or
exploding gradients).</p>

<p>Some methods have been proposed, notably <strong>batch normalization</strong> <sup id="fnref:3"><a href="#fn:3" class="footnote">1</a></sup> which
brings activations to zero-mean and unit variance, but they are perturbed by
stochastic regularisation methods (e.g. dropout <sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>).</p>

<p>Normalizing activations avoids a bias in the inputs of the following layer.</p>

<h2 id="how">How?</h2>

<p>An activation function is designed along the following requirements:</p>

<ol>
  <li>Negative and Positive values for controlling the mean</li>
  <li>Saturation regions (zero-gradient) to control the exploding gradient problem</li>
  <li>Slope larger than 1 to increase the variance if necessary</li>
  <li>Continuous curve</li>
</ol>

<p>Derive a mapping between layers that satisfies these requirements. Interestingly
the properties are verified through a computer-aided proof. The starting point
is mix between Exponential Linear Units (ELUs) and Leaky ReLUs.</p>

<script type="math/tex; mode=display">% <![CDATA[
selu(x) = \lambda  \begin{cases} x & \text{ if } x \gt 0 \\ \alpha e^{x} - \alpha & \text{ if } x \le 0 \end{cases} %]]></script>

<p>Parameters are obtained by finding a fixed point of the mapping function that
satisfies the normalization requirements. They propose:</p>

<p><script type="math/tex">\alpha = 1.6732632423543772848170429916717</script>
<script type="math/tex">\lambda = 1.0507009873554804934193349852946</script></p>

<p>They also derive a parametrised dropout which does not suffer from the same issues
as regular dropout as it is designed to preserve zero-mean and unit-variance in
the layer’s activations.</p>

<p><img src="/thoughts/assets/selu/selu.png" alt="SELU" />
<em>Figure 1: SELU activation function</em></p>

<h2 id="evaluation">Evaluation</h2>

<ul>
  <li>121 Tasks from <a href="https://archive.ics.uci.edu/ml/datasets.html">UCI dataset</a></li>
  <li>Drug discovery task</li>
  <li>Astronomy task</li>
</ul>

<p><img src="/thoughts/assets/selu/snn_accuracies.png" alt="Res" />
<em>Figure 2: Image from Klambauer et al, https://arxiv.org/abs/1706.02515 [license http://arxiv.org/licenses/nonexclusive-distrib/1.0/]</em></p>

<p>Compare against many baselines: Batch Norm, Layer Norm, Weight Norm, Highway, ResNet.</p>

<h2 id="comments">Comments</h2>

<ul>
  <li>Interesting use of a computer generated proof to give bounds on the variance
when weights don’t satisfy the zero-mean, unit-variance property</li>
  <li>Very smooth accuracy even for very deep networks (see Fig. 2)</li>
  <li>Sepp Hochreiter <sup id="fnref:1"><a href="#fn:1" class="footnote">3</a></sup></li>
  <li>Makes batchnorm obsolete, which means big training speedup</li>
</ul>

<h2 id="questions">Questions</h2>

<ul>
  <li>Evaluated with SGD. How does it behave with e.g. Adam?</li>
  <li>Evaluated with small learning rates, can we be more agressive in learning?</li>
  <li>Comparison between Spectral Normalization <sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup> and SNN for mode collapse in GANs.</li>
</ul>

<h2 id="resources">Resources</h2>
<h4 id="code">Code</h4>

<ul>
  <li><a href="https://github.com/bioinf-jku/SNNs">Tensorflow Implementation</a></li>
  <li><a href="https://github.com/dannysdeng/selu">PyTorch Implementation</a></li>
  <li><a href="https://gist.github.com/Drakensberge/2d8a4e8f9ff48e095e12a892b08598ec#file-distribution-ipynb">Notebook</a></li>
</ul>

<h4 id="discussion">Discussion</h4>

<ul>
  <li><a href="https://www.reddit.com/r/MachineLearning/comments/6g5tg1/r_selfnormalizing_neural_networks_improved_elu/">/r/MachineLearning</a></li>
  <li><a href="https://news.ycombinator.com/item?id=14527686">hackernews</a></li>
  <li><a href="http://www.shortscience.org/paper?bibtexKey=journals/corr/1706.02515">Shortscience</a></li>
</ul>

<h2 id="references">References</h2>

<div class="footnotes">
  <ol>
    <li id="fn:3">
      <p><a href="https://arxiv.org/abs/1502.03167"><em>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</em>, Sergey Ioffe, Christian Szegedy</a> <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p><a href="http://jmlr.org/papers/v15/srivastava14a.html"><em>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</em>, Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov; 15(Jun):1929−1958, 2014.</a> <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p><a href="https://dl.acm.org/citation.cfm?id=1246450"><em>Long Short-Term Memory</em>. Sepp Hochreiter and Jürgen Schmidhuber.  Neural Comput. 9, 8 (November 1997), 1735-1780</a> <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p><a href="https://openreview.net/forum?id=B1QRgziT-"><em>Spectral Normalization for Generative Adversarial Networks</em>, Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida</a> <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

          </div>

          <div class="article-share">
            
            
            <a href="https://twitter.com/home?status=%5BNOTES%5D+Self-Normalizing+Neural+Networks+%28SELU%29 - https://dtsbourg.github.io/thoughts/posts/self-normalizing-neural-nets by @dtsbourg" title="Share on Twitter" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M492 109.5c-17.4 7.7-36 12.9-55.6 15.3 20-12 35.4-31 42.6-53.6 -18.7 11.1-39.4 19.2-61.5 23.5C399.8 75.8 374.6 64 346.8 64c-53.5 0-96.8 43.4-96.8 96.9 0 7.6 0.8 15 2.5 22.1 -80.5-4-151.9-42.6-199.6-101.3 -8.3 14.3-13.1 31-13.1 48.7 0 33.6 17.2 63.3 43.2 80.7C67 210.7 52 206.3 39 199c0 0.4 0 0.8 0 1.2 0 47 33.4 86.1 77.7 95 -8.1 2.2-16.7 3.4-25.5 3.4 -6.2 0-12.3-0.6-18.2-1.8 12.3 38.5 48.1 66.5 90.5 67.3 -33.1 26-74.9 41.5-120.3 41.5 -7.8 0-15.5-0.5-23.1-1.4C62.8 432 113.7 448 168.3 448 346.6 448 444 300.3 444 172.2c0-4.2-0.1-8.4-0.3-12.5C462.6 146 479 129 492 109.5z"/></svg>
            </a>
            <a href="https://www.facebook.com/sharer/sharer.php?u=https://dtsbourg.github.io/thoughts/posts/self-normalizing-neural-nets" title="Share on Facebook" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M288 192v-38.1c0-17.2 3.8-25.9 30.5-25.9H352V64h-55.9c-68.5 0-91.1 31.4-91.1 85.3V192h-45v64h45v192h83V256h56.4l7.6-64H288z"/></svg>
            </a>
            <a href="https://plus.google.com/share?url=https://dtsbourg.github.io/thoughts/posts/self-normalizing-neural-nets" title="Share on Google+" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 128 128"><path d="M40.7 55.9v16.1c0 0 15.6 0 22 0C59.2 82.5 53.8 88.2 40.7 88.2c-13.3 0-23.7-10.8-23.7-24.2s10.4-24.2 23.7-24.2c7.1 0 11.6 2.5 15.8 5.9 3.3-3.3 3.1-3.8 11.6-11.9 -7.2-6.6-16.8-10.6-27.4-10.6C18.2 23.3 0 41.5 0 64c0 22.5 18.2 40.7 40.7 40.7 33.6 0 41.8-29.3 39-48.8H40.7zM113.9 56.7V42.6h-10.1v14.1H89.4v10.1h14.5v14.5h10.1V66.8H128V56.7H113.9z"/></svg>
            </a>
            <a href="http://news.ycombinator.com/submitlink?url=https://dtsbourg.github.io/thoughts/posts/self-normalizing-neural-nets" title="Share on HN" rel="noreferrer noopener" target="_blank">
                <svg xmlns="http://www.w3.org/2000/svg" aria-label="Hacker News" role="img" viewBox="0 0 512 512"><rect width="512" height="512" fill="#ADA8A8" rx="15%"/><path fill="#fff" d="M124 91h51l81 162 81-164h51L276 293v136h-40V293z"/></svg>            
            </a>
          </div>

          
        </article>
        <footer class="footer appear">
  <p>
    I'm
    <a href="https://dtsbourg.github.io" title="About me">Dylan Bourgeois</a>. I study robots and their brains.
  </p>
</footer>

      </div>
    </div>
  </main>
  
  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-65856095-1','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async></script>


<script src="/thoughts/assets/vendor.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



  <script src="/thoughts/assets/webfonts.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>




  <script src="/thoughts/assets/scrollappear.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script src="/thoughts/assets/application.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
